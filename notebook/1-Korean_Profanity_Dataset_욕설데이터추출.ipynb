{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f622ee-34dd-477b-ad49-c69a99769309",
   "metadata": {},
   "source": [
    "## 한국어 욕설 데이터 추출\n",
    "\n",
    "- 데이터 원본 : LLM 학습용 데이터 내 유해표현 검출 AI모델 학습용 데이터\n",
    "- https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=71833\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5abec6e7-da17-4bb8-98b9-6adf284bb892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "amzn2-core                                               | 3.6 kB     00:00     \n",
      "amzn2extra-docker                                        | 2.9 kB     00:00     \n",
      "amzn2extra-kernel-5.10                                   | 3.0 kB     00:00     \n",
      "amzn2extra-livepatch                                     | 2.9 kB     00:00     \n",
      "amzn2extra-lustre                                        | 2.5 kB     00:00     \n",
      "amzn2extra-python3.8                                     | 2.9 kB     00:00     \n",
      "centos-extras                                            | 2.9 kB     00:00     \n",
      "copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxi | 3.3 kB     00:00     \n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "nvidia-container-toolkit/x86_64/signature                |  833 B     00:00     \n",
      "nvidia-container-toolkit/x86_64/signature                | 2.1 kB     00:01 !!! \n",
      "nvidia-container-toolkit/x86_64/primary                    |  24 kB   00:00     \n",
      "nvidia-container-toolkit                                                155/155\n",
      "63 packages excluded due to repository priority protections\n",
      "Resolving Dependencies\n",
      "--> Running transaction check\n",
      "---> Package java-1.8.0-openjdk-devel.x86_64 1:1.8.0.452.b09-1.amzn2.0.1 will be installed\n",
      "--> Processing Dependency: java-1.8.0-openjdk(x86-64) = 1:1.8.0.452.b09-1.amzn2.0.1 for package: 1:java-1.8.0-openjdk-devel-1.8.0.452.b09-1.amzn2.0.1.x86_64\n",
      "--> Processing Dependency: libjava.so()(64bit) for package: 1:java-1.8.0-openjdk-devel-1.8.0.452.b09-1.amzn2.0.1.x86_64\n",
      "--> Processing Dependency: libjvm.so()(64bit) for package: 1:java-1.8.0-openjdk-devel-1.8.0.452.b09-1.amzn2.0.1.x86_64\n",
      "--> Running transaction check\n",
      "---> Package java-1.8.0-openjdk.x86_64 1:1.8.0.452.b09-1.amzn2.0.1 will be installed\n",
      "--> Processing Dependency: xorg-x11-fonts-Type1 for package: 1:java-1.8.0-openjdk-1.8.0.452.b09-1.amzn2.0.1.x86_64\n",
      "---> Package java-1.8.0-openjdk-headless.x86_64 1:1.8.0.452.b09-1.amzn2.0.1 will be installed\n",
      "--> Processing Dependency: copy-jdk-configs >= 3.3 for package: 1:java-1.8.0-openjdk-headless-1.8.0.452.b09-1.amzn2.0.1.x86_64\n",
      "--> Processing Dependency: lksctp-tools(x86-64) for package: 1:java-1.8.0-openjdk-headless-1.8.0.452.b09-1.amzn2.0.1.x86_64\n",
      "--> Processing Dependency: pcsc-lite-libs(x86-64) for package: 1:java-1.8.0-openjdk-headless-1.8.0.452.b09-1.amzn2.0.1.x86_64\n",
      "--> Running transaction check\n",
      "---> Package copy-jdk-configs.noarch 0:3.3-10.amzn2 will be installed\n",
      "---> Package lksctp-tools.x86_64 0:1.0.17-2.amzn2.0.2 will be installed\n",
      "---> Package pcsc-lite-libs.x86_64 0:1.8.8-7.amzn2 will be installed\n",
      "---> Package xorg-x11-fonts-Type1.noarch 0:7.5-9.amzn2 will be installed\n",
      "--> Processing Dependency: ttmkfdir for package: xorg-x11-fonts-Type1-7.5-9.amzn2.noarch\n",
      "--> Processing Dependency: ttmkfdir for package: xorg-x11-fonts-Type1-7.5-9.amzn2.noarch\n",
      "--> Running transaction check\n",
      "---> Package ttmkfdir.x86_64 0:3.0.9-42.amzn2.0.2 will be installed\n",
      "--> Finished Dependency Resolution\n",
      "\n",
      "Dependencies Resolved\n",
      "\n",
      "================================================================================\n",
      " Package                    Arch   Version                     Repository  Size\n",
      "================================================================================\n",
      "Installing:\n",
      " java-1.8.0-openjdk-devel   x86_64 1:1.8.0.452.b09-1.amzn2.0.1 amzn2-core 9.8 M\n",
      "Installing for dependencies:\n",
      " copy-jdk-configs           noarch 3.3-10.amzn2                amzn2-core  21 k\n",
      " java-1.8.0-openjdk         x86_64 1:1.8.0.452.b09-1.amzn2.0.1 amzn2-core 322 k\n",
      " java-1.8.0-openjdk-headless\n",
      "                            x86_64 1:1.8.0.452.b09-1.amzn2.0.1 amzn2-core  33 M\n",
      " lksctp-tools               x86_64 1.0.17-2.amzn2.0.2          amzn2-core  88 k\n",
      " pcsc-lite-libs             x86_64 1.8.8-7.amzn2               amzn2-core  35 k\n",
      " ttmkfdir                   x86_64 3.0.9-42.amzn2.0.2          amzn2-core  50 k\n",
      " xorg-x11-fonts-Type1       noarch 7.5-9.amzn2                 amzn2-core 521 k\n",
      "\n",
      "Transaction Summary\n",
      "================================================================================\n",
      "Install  1 Package (+7 Dependent packages)\n",
      "\n",
      "Total download size: 44 M\n",
      "Installed size: 153 M\n",
      "Downloading packages:\n",
      "(1/8): copy-jdk-configs-3.3-10.amzn2.noarch.rpm            |  21 kB   00:00     \n",
      "(2/8): java-1.8.0-openjdk-1.8.0.452.b09-1.amzn2.0.1.x86_64 | 322 kB   00:00     \n",
      "(3/8): java-1.8.0-openjdk-devel-1.8.0.452.b09-1.amzn2.0.1. | 9.8 MB   00:00     \n",
      "(4/8): lksctp-tools-1.0.17-2.amzn2.0.2.x86_64.rpm          |  88 kB   00:00     \n",
      "(5/8): pcsc-lite-libs-1.8.8-7.amzn2.x86_64.rpm             |  35 kB   00:00     \n",
      "(6/8): ttmkfdir-3.0.9-42.amzn2.0.2.x86_64.rpm              |  50 kB   00:00     \n",
      "(7/8): xorg-x11-fonts-Type1-7.5-9.amzn2.noarch.rpm         | 521 kB   00:00     \n",
      "(8/8): java-1.8.0-openjdk-headless-1.8.0.452.b09-1.amzn2.0 |  33 MB   00:00     \n",
      "--------------------------------------------------------------------------------\n",
      "Total                                               91 MB/s |  44 MB  00:00     \n",
      "Running transaction check\n",
      "Running transaction test\n",
      "Transaction test succeeded\n",
      "Running transaction\n",
      "Warning: RPMDB altered outside of yum.\n",
      "  Installing : pcsc-lite-libs-1.8.8-7.amzn2.x86_64                          1/8 \n",
      "  Installing : copy-jdk-configs-3.3-10.amzn2.noarch                         2/8 \n",
      "  Installing : ttmkfdir-3.0.9-42.amzn2.0.2.x86_64                           3/8 \n",
      "  Installing : xorg-x11-fonts-Type1-7.5-9.amzn2.noarch                      4/8 \n",
      "  Installing : lksctp-tools-1.0.17-2.amzn2.0.2.x86_64                       5/8 \n",
      "  Installing : 1:java-1.8.0-openjdk-headless-1.8.0.452.b09-1.amzn2.0.1.x8   6/8 \n",
      "  Installing : 1:java-1.8.0-openjdk-1.8.0.452.b09-1.amzn2.0.1.x86_64        7/8 \n",
      "  Installing : 1:java-1.8.0-openjdk-devel-1.8.0.452.b09-1.amzn2.0.1.x86_6   8/8 \n",
      "  Verifying  : xorg-x11-fonts-Type1-7.5-9.amzn2.noarch                      1/8 \n",
      "  Verifying  : 1:java-1.8.0-openjdk-headless-1.8.0.452.b09-1.amzn2.0.1.x8   2/8 \n",
      "  Verifying  : lksctp-tools-1.0.17-2.amzn2.0.2.x86_64                       3/8 \n",
      "  Verifying  : ttmkfdir-3.0.9-42.amzn2.0.2.x86_64                           4/8 \n",
      "  Verifying  : 1:java-1.8.0-openjdk-1.8.0.452.b09-1.amzn2.0.1.x86_64        5/8 \n",
      "  Verifying  : copy-jdk-configs-3.3-10.amzn2.noarch                         6/8 \n",
      "  Verifying  : pcsc-lite-libs-1.8.8-7.amzn2.x86_64                          7/8 \n",
      "  Verifying  : 1:java-1.8.0-openjdk-devel-1.8.0.452.b09-1.amzn2.0.1.x86_6   8/8 \n",
      "\n",
      "Installed:\n",
      "  java-1.8.0-openjdk-devel.x86_64 1:1.8.0.452.b09-1.amzn2.0.1                   \n",
      "\n",
      "Dependency Installed:\n",
      "  copy-jdk-configs.noarch 0:3.3-10.amzn2                                        \n",
      "  java-1.8.0-openjdk.x86_64 1:1.8.0.452.b09-1.amzn2.0.1                         \n",
      "  java-1.8.0-openjdk-headless.x86_64 1:1.8.0.452.b09-1.amzn2.0.1                \n",
      "  lksctp-tools.x86_64 0:1.0.17-2.amzn2.0.2                                      \n",
      "  pcsc-lite-libs.x86_64 0:1.8.8-7.amzn2                                         \n",
      "  ttmkfdir.x86_64 0:3.0.9-42.amzn2.0.2                                          \n",
      "  xorg-x11-fonts-Type1.noarch 0:7.5-9.amzn2                                     \n",
      "\n",
      "Complete!\n",
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading jpype1-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting lxml>=4.1.0 (from konlpy)\n",
      "  Downloading lxml-5.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
      "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jpype1-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
      "Downloading lxml-5.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m196.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, JPype1, konlpy\n",
      "Successfully installed JPype1-1.5.2 konlpy-0.6.0 lxml-5.4.0\n",
      "Collecting JPype1==1.3.0\n",
      "  Downloading JPype1-1.3.0.tar.gz (820 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m820.3/820.3 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: JPype1\n",
      "  Building wheel for JPype1 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for JPype1: filename=jpype1-1.3.0-cp310-cp310-linux_x86_64.whl size=443525 sha256=7ae14df8e8d5c30c88e10174e641eeb7fbb74713599ea097b7cab09d877a75ca\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/f5/c7/8f/c97c6c9868c256c8d17dabb772a2ca9002dcff2912fa8d7d58\n",
      "Successfully built JPype1\n",
      "Installing collected packages: JPype1\n",
      "  Attempting uninstall: JPype1\n",
      "    Found existing installation: jpype1 1.5.2\n",
      "    Uninstalling jpype1-1.5.2:\n",
      "      Successfully uninstalled jpype1-1.5.2\n",
      "Successfully installed JPype1-1.3.0\n"
     ]
    }
   ],
   "source": [
    "# KoNLPy 및 JPype 설치 Java 설치 (Amazon Linux 기반)\n",
    "!sudo yum install -y java-1.8.0-openjdk-devel\n",
    "!pip install konlpy\n",
    "!pip install JPype1==1.3.0  # 최신 버전이 호환성 문제가 있을 수 있어 안정 버전 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677aa07-1479-4956-ba70-1b1de1342ca0",
   "metadata": {},
   "source": [
    "### 유해 카테고리 분류 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e93430ab-92fc-424b-baf3-8ce8f7e77039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage1_data의 결측치 확인\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "모욕           0\n",
       "욕설           0\n",
       "외설           0\n",
       "폭력위협/범죄조장    0\n",
       "성혐오          0\n",
       "연령           0\n",
       "인종/지역        0\n",
       "장애           0\n",
       "종교           0\n",
       "정치성향         0\n",
       "직업           0\n",
       "정상           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#stage2_data.csv 도 로드하는 코드\n",
    "stage2_data = pd.read_csv('data/2.harmful_category_classification/stage2_data.csv', index_col=0)\n",
    "\n",
    "print(\"stage1_data의 결측치 확인\")\n",
    "stage2_data.isnull().sum() #결측치 확인\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25447d66-53d7-449e-8c0b-7690d91155be",
   "metadata": {},
   "source": [
    "### 추출용 유틸리티함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dec8eb8b-de3c-416f-85f9-78796f369cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 따옴표 정리 함수\n",
    "def clean_quotes(text):\n",
    "    if isinstance(text, str):\n",
    "        # 여러 중첩된 따옴표를 하나의 따옴표로 정리\n",
    "        cleaned_text = re.sub(r\"'{2,}\", \"'\", text)  # 작은따옴표 정리\n",
    "        cleaned_text = re.sub(r'\"{2,}', '\"', cleaned_text)  # 큰따옴표 정리\n",
    "        \n",
    "        # 필요하다면 따옴표를 완전히 제거\n",
    "        # cleaned_text = re.sub(r\"['\\\"]\", \"\", cleaned_text)\n",
    "        \n",
    "        return cleaned_text\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_all_quotes(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 모든 종류의 따옴표를 제거하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): 따옴표를 제거할 원본 텍스트\n",
    "    \n",
    "    Returns:\n",
    "    str: 따옴표가 제거된 텍스트\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # 작은따옴표('), 큰따옴표(\") 모두 제거\n",
    "    cleaned_text = re.sub(r\"['\\\"]\", \"\", text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# DataFrame에 적용하는 예시\n",
    "# stage2_data['문장_정제'] = stage2_data['문장'].apply(remove_all_quotes)\n",
    "\n",
    "\n",
    "def remove_quotes_and_special_chars(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 따옴표와 특수문자를 제거하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): 처리할 원본 텍스트\n",
    "    \n",
    "    Returns:\n",
    "    str: 따옴표와 특수문자가 제거된 텍스트\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # 1. 따옴표(', \") 제거\n",
    "    text = re.sub(r\"['\\\"]\", \"\", text)\n",
    "    \n",
    "    # 2. 백틱(`) 제거\n",
    "    text = re.sub(r\"`\", \"\", text)\n",
    "    \n",
    "    # 3. 기타 일반적인 특수문자 제거 (필요에 따라 조정 가능)\n",
    "    # 여기서는 백틱, 역따옴표, 중괄호, 대괄호, 작은/큰 괄호 등을 제거\n",
    "    text = re.sub(r\"[`´''\"\"『』\\[\\](){}]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 확장 버전 - 더 많은 특수문자 제거가 필요한 경우\n",
    "def remove_all_special_chars(text, keep_korean=True, keep_english=True, \n",
    "                             keep_numbers=True, keep_spaces=True):\n",
    "    \"\"\"\n",
    "    텍스트에서 선택적으로 특수문자를 제거하는 확장 함수\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): 처리할 원본 텍스트\n",
    "    keep_korean (bool): 한글 유지 여부\n",
    "    keep_english (bool): 영어 유지 여부\n",
    "    keep_numbers (bool): 숫자 유지 여부\n",
    "    keep_spaces (bool): 공백 유지 여부\n",
    "    \n",
    "    Returns:\n",
    "    str: 특수문자가 제거된 텍스트\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    pattern = \"\"\n",
    "    \n",
    "    # 유지할 문자 패턴 설정\n",
    "    if keep_korean:\n",
    "        pattern += \"가-힣\"\n",
    "    if keep_english:\n",
    "        pattern += \"a-zA-Z\"\n",
    "    if keep_numbers:\n",
    "        pattern += \"0-9\"\n",
    "    if keep_spaces:\n",
    "        pattern += \" \"\n",
    "    \n",
    "    # 지정된 문자 외 모든 것 제거\n",
    "    return re.sub(f\"[^{pattern}]\", \"\", text)\n",
    "    \n",
    "def extract_quoted_words(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 따옴표(', \", `, 작은/큰따옴표 등) 안에 있는 단어나 구문을 추출하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): 처리할 원본 텍스트\n",
    "    \n",
    "    Returns:\n",
    "    list: 따옴표 안에 있던 단어/구문 목록\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # 여러 유형의 따옴표 패턴 정의\n",
    "    patterns = [\n",
    "        r\"'([^']+)'\",         # 작은따옴표: '단어'\n",
    "        r'\"([^\"]+)\"',         # 큰따옴표: \"단어\"\n",
    "        r\"`([^`]+)`\",         # 백틱: `단어`\n",
    "        r\"''([^']+)''\",       # 이중 작은따옴표: ''단어''\n",
    "        r\"'''([^']+)'''\",     # 삼중 작은따옴표: '''단어'''\n",
    "        r\"『([^』]+)』\",       # 한국어 인용부호: 『단어』\n",
    "        r\"「([^」]+)」\"        # 한국어 인용부호: 「단어」\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        results.extend(matches)\n",
    "    \n",
    "    # 중복 제거 및 공백 제거\n",
    "    results = [word.strip() for word in results]\n",
    "    results = list(set(results))  # 중복 제거\n",
    "    \n",
    "    # AWS Guardrail 제한사항: 최대 3단어까지만 허용\n",
    "    results = [word for word in results if len(word.split()) <= 3]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# DataFrame에 적용하여 차단 단어 목록 생성\n",
    "def create_block_list_from_dataframe(df, text_column):\n",
    "    \"\"\"\n",
    "    DataFrame의 텍스트 열에서 따옴표 안의 단어를 추출하여 차단 목록 생성\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): 텍스트 데이터가 있는 DataFrame\n",
    "    text_column (str): 텍스트가 있는 열 이름\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: 차단 단어 목록이 있는 DataFrame\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    \n",
    "    # 각 텍스트에서 따옴표 안의 단어 추출\n",
    "    for text in df[text_column]:\n",
    "        extracted_words = extract_quoted_words(text)\n",
    "        all_words.extend(extracted_words)\n",
    "    \n",
    "    # 중복 제거 및 정렬\n",
    "    all_words = list(set(all_words))\n",
    "    all_words.sort()\n",
    "    \n",
    "    # 차단 목록 DataFrame 생성 (AWS Guardrail 형식에 맞게)\n",
    "    block_list_df = pd.DataFrame(all_words, columns=['blocked_words'])\n",
    "    \n",
    "    return block_list_df\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# 기존 함수는 이미 중복 제거 기능이 포함되어 있지만, 더 강화된 중복 처리 함수를 작성합니다\n",
    "def find_and_merge_duplicates(block_list_df):\n",
    "    \"\"\"\n",
    "    차단 목록에서 중복 또는 유사 단어를 찾아 병합하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    block_list_df (DataFrame): 차단 단어 목록이 있는 DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: 중복이 제거된 차단 단어 목록 DataFrame\n",
    "    \"\"\"\n",
    "    # 1. 기본 전처리\n",
    "    words = block_list_df['blocked_words'].tolist()\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # 공백 표준화 (여러 공백을 하나로)\n",
    "        word = re.sub(r'\\s+', ' ', word).strip()\n",
    "        processed_words.append(word)\n",
    "    \n",
    "    # 2. 정확한 중복 제거 (대소문자 무시)\n",
    "    unique_words = []\n",
    "    unique_words_lower = []\n",
    "    \n",
    "    for word in processed_words:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower not in unique_words_lower:\n",
    "            unique_words.append(word)\n",
    "            unique_words_lower.append(word_lower)\n",
    "    \n",
    "    # 3. 유사 단어 찾기 및 병합 (선택 사항)\n",
    "    similar_groups = []\n",
    "    used_indices = set()\n",
    "    \n",
    "    # 유사도 기준 (0.8은 80% 유사함을 의미)\n",
    "    similarity_threshold = 0.8\n",
    "    \n",
    "    for i in range(len(unique_words)):\n",
    "        if i in used_indices:\n",
    "            continue\n",
    "            \n",
    "        group = [unique_words[i]]\n",
    "        used_indices.add(i)\n",
    "        \n",
    "        for j in range(i+1, len(unique_words)):\n",
    "            if j in used_indices:\n",
    "                continue\n",
    "                \n",
    "            # 유사도 계산 (Levenshtein 거리 기반)\n",
    "            similarity = SequenceMatcher(None, \n",
    "                                         unique_words[i].lower(), \n",
    "                                         unique_words[j].lower()).ratio()\n",
    "            \n",
    "            if similarity >= similarity_threshold:\n",
    "                group.append(unique_words[j])\n",
    "                used_indices.add(j)\n",
    "        \n",
    "        if len(group) > 1:  # 유사한 단어가 발견된 경우\n",
    "            similar_groups.append(group)\n",
    "    \n",
    "    # 4. 최종 결과 생성\n",
    "    final_words = []\n",
    "    \n",
    "    # 유사 그룹에 포함되지 않은 단어 추가\n",
    "    for i, word in enumerate(unique_words):\n",
    "        found_in_group = False\n",
    "        for group in similar_groups:\n",
    "            if word in group:\n",
    "                found_in_group = True\n",
    "                break\n",
    "        \n",
    "        if not found_in_group:\n",
    "            final_words.append(word)\n",
    "    \n",
    "    # 유사 그룹의 대표 단어 추가 (가장 짧은 단어 선택)\n",
    "    for group in similar_groups:\n",
    "        # 가장 짧은 단어를 대표로 선택 (또는 다른 기준 적용 가능)\n",
    "        representative = min(group, key=len)\n",
    "        final_words.append(representative)\n",
    "        \n",
    "        # 유사 단어 그룹 출력 (참고용)\n",
    "        print(f\"유사 단어 그룹 병합: {group} -> {representative}\")\n",
    "    \n",
    "    # 최종 DataFrame 생성\n",
    "    final_df = pd.DataFrame(final_words, columns=['blocked_words'])\n",
    "    \n",
    "    # 정렬 적용\n",
    "    final_df = final_df.sort_values('blocked_words').reset_index(drop=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def filter_common_words(block_list_df):\n",
    "    \"\"\"\n",
    "    차단 목록에서 일반적인 단어/조사/접속사 등을 제거하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    block_list_df (DataFrame): 차단 단어 목록이 있는 DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: 일반 단어가 제거된 차단 단어 목록 DataFrame\n",
    "    \"\"\"\n",
    "    # 한국어 불용어 목록 (필요에 따라 확장)\n",
    "    korean_stopwords = [\n",
    "        # 일반 접속사/조사\n",
    "        '및', '등', '과', '와', '의', '에', '에서', '로', '으로', '에게', '께', '한', '을', '를', '이', '가',\n",
    "        \n",
    "        # 일반 단어\n",
    "        '것', '수', '듯', '때', '곳', '중', '내', '외', '앞', '뒤', '위', '아래', '좌', '우', \n",
    "        \n",
    "        # 지시어\n",
    "        '이', '그', '저', '이것', '그것', '저것', '여기', '저기', '거기',\n",
    "        \n",
    "        # 시간 관련\n",
    "        '때', '지금', '현재', '오늘', '내일', '어제', '다음', '이전',\n",
    "        \n",
    "        # 숫자 관련\n",
    "        '하나', '둘', '셋', '넷', '다섯', '첫째', '둘째', '셋째',\n",
    "        \n",
    "        # 의문사\n",
    "        '무엇', '어디', '언제', '누구', '어떤', '어떻게', '왜', '얼마나',\n",
    "        \n",
    "        # 기타 일반 단어\n",
    "        '정도', '경우', '상태', '모두', '모든', '각', '각각', '서로', '우리', '그들', '모든'\n",
    "    ]\n",
    "    \n",
    "    # 너무 짧은 단어 제외 (1글자)\n",
    "    short_words = [word for word in block_list_df['blocked_words'] if len(word) == 1]\n",
    "    \n",
    "    # 불용어 목록과 짧은 단어 목록을 합침\n",
    "    words_to_exclude = set(korean_stopwords + short_words)\n",
    "    \n",
    "    # 필터링 전 개수\n",
    "    original_count = len(block_list_df)\n",
    "    \n",
    "    # 불용어 제외\n",
    "    filtered_df = block_list_df[~block_list_df['blocked_words'].isin(words_to_exclude)]\n",
    "    \n",
    "    # 제외된 단어 목록 출력\n",
    "    excluded_words = set(block_list_df['blocked_words']) & words_to_exclude\n",
    "    print(f\"제외된 일반 단어/불용어 ({len(excluded_words)}개):\")\n",
    "    print(sorted(list(excluded_words)))\n",
    "    \n",
    "    # 필터링 후 개수\n",
    "    filtered_count = len(filtered_df)\n",
    "    print(f\"\\n필터링 전: {original_count}개 단어\")\n",
    "    print(f\"필터링 후: {filtered_count}개 단어\")\n",
    "    print(f\"제거된 단어 수: {original_count - filtered_count}개\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_special_quote_pattern(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 ''로 시작하고 '''로 끝나는 패턴 내부의 내용을 추출하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): 처리할 원본 텍스트\n",
    "    \n",
    "    Returns:\n",
    "    list: 추출된 내용 목록\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # ''로 시작하고 '''로 끝나는 패턴을 찾는 정규표현식\n",
    "    pattern = r\"''([^']*)'''\"\n",
    "    \n",
    "    # 패턴에 맞는 모든 내용 추출\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def remove_single_char_words(block_list_df):\n",
    "    \"\"\"\n",
    "    차단 목록에서 1글자로 된 단어를 제거하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    block_list_df (DataFrame): 차단 단어 목록이 있는 DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: 1글자 단어가 제거된 차단 단어 목록 DataFrame\n",
    "    \"\"\"\n",
    "    # 필터링 전 단어 수\n",
    "    original_count = len(block_list_df)\n",
    "    \n",
    "    # 단어 길이 계산 함수 (공백 제외)\n",
    "    def get_effective_length(word):\n",
    "        if not isinstance(word, str):\n",
    "            return 0\n",
    "        # 공백 제거 후 길이 계산\n",
    "        return len(word.replace(\" \", \"\"))\n",
    "    \n",
    "    # 1글자 단어 찾기\n",
    "    single_char_words = block_list_df[block_list_df['blocked_words'].apply(get_effective_length) <= 1]\n",
    "    \n",
    "    # 1글자 단어 제거하기 (effective length가 2 이상인 단어만 유지)\n",
    "    filtered_df = block_list_df[block_list_df['blocked_words'].apply(get_effective_length) > 1]\n",
    "    \n",
    "    # 필터링 후 단어 수\n",
    "    filtered_count = len(filtered_df)\n",
    "    \n",
    "    # 제거된 단어 출력\n",
    "    if not single_char_words.empty:\n",
    "        print(f\"제거된 1글자 단어 ({len(single_char_words)}개):\")\n",
    "        #for word in single_char_words['blocked_words'].tolist():\n",
    "            #print(f\"- '{word}'\")\n",
    "    \n",
    "    print(f\"\\n필터링 전: {original_count}개 단어\")\n",
    "    print(f\"필터링 후: {filtered_count}개 단어\")\n",
    "    print(f\"제거된 단어 수: {original_count - filtered_count}개\")\n",
    "    \n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0395d-9729-41f6-89b0-56de026430ef",
   "metadata": {},
   "source": [
    "### 칼럼명 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf111a6c-b58f-449f-a630-6625ebd5b2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['모욕', '욕설', '외설', '폭력위협/범죄조장', '성혐오', '연령', '인종/지역', '장애', '종교', '정치성향',\n",
      "       '직업', '정상'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(stage2_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5240e6d9-52bc-4a14-9f07-f044707b928d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['독점은 당연히 막아야지. 이건 사장경제는 경쟁으로 가야 정상순환되는데 독점이 되면 기업이 공산국가처럼 독재하는건데',\n",
      "       '눈 작다고 한 마디 한 걸로 퇴출시키는 그리스가 훨씬 선진국이네 ㅋㅋㅋ MBC는 사과문 ''끄적'''이고 ''버로우 타'''고 있는데',\n",
      "       '''도쿄만 똥물'''도 문제지만 해안에 모래는 후쿠시마 흙으로 채웠는데 이선수들 피폭이 걱정된다',\n",
      "       '''종북''' ''좌파''' ''빨갱이'''들의 마지막 부정선거가 다음 대선이다 . ' 미군 철수 , 국가보안법 폐지 ' 아프간 사태는 우리에게 보내는 마지막 경고 메세지다 .',\n",
      "       '동료 여경 집단 성희롱’ 태백 경찰관들 12일 징계 논의-----''예비강간마'''''새끼'''들 기사 묻히는 이유 좀 알려주세요',\n",
      "       '일제시대때 ''친일파'''  ''매국노'''들과 같네 .''토왜'''들도 저리해서 잘먹고 잘 살았겠지?',\n",
      "       '''쩡개'''꺼눈 특별히 메드인차이나 재품으로 준곳임',\n",
      "       '빨리 일반 질병으로 분류합시다. 현명한 결정이라고 봅니다. 제약사들 ''비지니스에 놀아나'''지 맙시다.',\n",
      "       '여기저기 ''정신병 걸린'''애들이 많긴많구나~1950년 전 대한제국인줄 아나?',\n",
      "       '없던 질문을 자막에 넣은 mbc, 과거 ''광우뻥''' 사태 때도 실체가 없는 것을 진실인양 보도해서 나라를 뒤집었지만 사과 한마디 없음~',\n",
      "       ...\n",
      "       ' 따뜻하게 세상을 바라보는 아이처럼, `루카` 속 풍경은 평화롭다',\n",
      "       '온갖 고문으로 오기섭이 탈진해 있을 때 최달식은 자기 집으로 전화를 걸어 아들과 통화한다',\n",
      "       '중소·중견기업의 경우 해당 업종의 통계를 입증하기가 쉽지 않고 서비스업의 경우 보조지표중 ‘고용대비 서비스생산 지수 ’외에는 활용할 만한 지표가 없어 기업들이 신청을 주저할 수 있다는 내용이었다',\n",
      "       '\"시 쓰기가 고통스러웠던 적은 없어요',\n",
      "       '심지어 다국적 기업이 미국 외 지역에서 얻은 이익 중에는 `무국적 항목`으로 분류되는 것도 있다',\n",
      "       ' 문체부는 세제 혜택을 받는 일부 대중 골프장이 과도한 이용료, 캐디·카트 강제 이용 등을 요구하는 대중 친화적이지 않은 영업 문제를 해결하기 위해 골프장을 세분화했다',\n",
      "       'VDT 증후군 수진자 수는 2009년 458만명에서 2012년 553만명, 지난해 634만명으로 늘었다',\n",
      "       ' 카우스 대표는 '사무실 문화에 종말이 왔는가'라는 질문에 고개를 저었다',\n",
      "       '영기는 `블소2` 게임 속에서 추가 경험치 획득률 증가 추가 재화 획득률 증가 비각인(거래 가능) 아이템 획득 가능 효과를 부여하는 시스템이다',\n",
      "       '전문가들은 부동산 세금정책이 공급 확대로 이어지려면 보유세가 아닌 양도소득세를 완화해야 한다고 입을 모았다'],\n",
      "      dtype='object', name='문장', length=200000)\n",
      "0    독점은 당연히 막아야지. 이건 사장경제는 경쟁으로 가야 정상순환되는데 독점이 되면 ...\n",
      "1    눈 작다고 한 마디 한 걸로 퇴출시키는 그리스가 훨씬 선진국이네 ㅋㅋㅋ MBC는 사...\n",
      "2    ''도쿄만 똥물'''도 문제지만 해안에 모래는 후쿠시마 흙으로 채웠는데 이선수들 피...\n",
      "3    ''종북''' ''좌파''' ''빨갱이'''들의 마지막 부정선거가 다음 대선이다 ....\n",
      "4    동료 여경 집단 성희롱’ 태백 경찰관들 12일 징계 논의-----''예비강간마'''...\n",
      "Name: 문장, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(stage2_data.index)\n",
    "#2. 만약 인덱스가 문장이라면 인덱스를 열로 변환하기\n",
    "# 인덱스를 '문장' 열로 변환\n",
    "stage2_data.reset_index(inplace=True)\n",
    "stage2_data.rename(columns={'index': '문장'}, inplace=True)\n",
    "\n",
    "# 이제 문장 열에 접근 가능\n",
    "print(stage2_data['문장'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8fed80c-2ae1-4fb7-a531-7611cc53ec39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['문장', '모욕', '욕설', '외설', '폭력위협/범죄조장', '성혐오', '연령', '인종/지역', '장애', '종교',\n",
      "       '정치성향', '직업', '정상'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(stage2_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29486440-4982-4fa5-a645-930ba2f67c77",
   "metadata": {},
   "source": [
    "## DataFrame에 적용하여 차단 목록 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "031e2442-c23e-422f-91e5-8514be6fb34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame에 적용하여 차단 목록 생성\n",
    "def create_block_list_from_special_quotes(df, text_column):\n",
    "    \"\"\"\n",
    "    DataFrame의 텍스트 열에서 특수 따옴표 패턴 내 단어를 추출하여 차단 목록 생성\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): 텍스트 데이터가 있는 DataFrame\n",
    "    text_column (str): 텍스트가 있는 열 이름\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: 차단 단어 목록이 있는 DataFrame\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    \n",
    "    # 각 텍스트에서 특수 따옴표 패턴 내 단어 추출\n",
    "    for text in df[text_column]:\n",
    "        if isinstance(text, str):\n",
    "            extracted_words = extract_special_quote_pattern(text)\n",
    "            all_words.extend(extracted_words)\n",
    "    \n",
    "    # 중복 제거 및 정렬\n",
    "    all_words = list(set(all_words))\n",
    "    all_words.sort()\n",
    "    \n",
    "    # 차단 목록 DataFrame 생성\n",
    "    block_list_df = pd.DataFrame(all_words, columns=['blocked_words'])\n",
    "    \n",
    "    return block_list_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f5ad5-408a-4a49-a2a9-605a8bf3bba1",
   "metadata": {},
   "source": [
    "### 특정 욕설의 값이 있는 카테고리 지정\n",
    "\n",
    "- 아래 예시는 폭력위협/범죄조장을 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3688c7ed-d1d0-4d1c-9a52-89c118153366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>모욕</th>\n",
       "      <th>욕설</th>\n",
       "      <th>외설</th>\n",
       "      <th>폭력위협/범죄조장</th>\n",
       "      <th>성혐오</th>\n",
       "      <th>연령</th>\n",
       "      <th>인종/지역</th>\n",
       "      <th>장애</th>\n",
       "      <th>종교</th>\n",
       "      <th>정치성향</th>\n",
       "      <th>직업</th>\n",
       "      <th>정상</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>''Cㅂ ㅏ''' ''ㅇ ㅏㄱ ㅏㅃ ㅏ 뤼''' 다 ''트자 뿔''' ㄹ ㅏ</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>이래서 ''더불어모화당'''은 중국으로 ''추방해'''야..</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>''강간범'''한테 ''폭행'''''강간'''당한 나는 저런모습이엇을까 ㅋㅋ</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>브라질 흑인 **번은 가발쓴 남자가 분명하다.도핑이고 돌핀이고 간에 성별검사부터 해라</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>조선과 명나라 연합군이 일본 정벌하러 가다 태풍만 만나지 않았더라면...그때 확실하...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99789</th>\n",
       "      <td>#@이름#들 데이트폭력 근절해주세요 특히 애인 아내 죽이는 #@이름#에게 사형을 주...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99795</th>\n",
       "      <td>''한남'''들은 페미, ''메갈''' 이런것도 안하는데 말하는거 들으면 ''토악질...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99839</th>\n",
       "      <td>''헬조선''' ''새끼'''들은 인종차별이라고 왜 ''씨부리'''는지? 니네들도 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99905</th>\n",
       "      <td>시청자들이 ''갑'''인데 ''갑'''한테 ''그따구'''로 고지도 안하고 ''쳐맞...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99928</th>\n",
       "      <td>놀고 ''자빠졌'''네ㅎㅎ#@이름# 콱! ''때려뿔'''라~</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6559 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      문장  모욕  욕설  외설  \\\n",
       "38            ''Cㅂ ㅏ''' ''ㅇ ㅏㄱ ㅏㅃ ㅏ 뤼''' 다 ''트자 뿔''' ㄹ ㅏ   0   2   0   \n",
       "44                     이래서 ''더불어모화당'''은 중국으로 ''추방해'''야..   0   0   0   \n",
       "53            ''강간범'''한테 ''폭행'''''강간'''당한 나는 저런모습이엇을까 ㅋㅋ   2   0   0   \n",
       "74       브라질 흑인 **번은 가발쓴 남자가 분명하다.도핑이고 돌핀이고 간에 성별검사부터 해라   2   0   0   \n",
       "79     조선과 명나라 연합군이 일본 정벌하러 가다 태풍만 만나지 않았더라면...그때 확실하...   0   0   0   \n",
       "...                                                  ...  ..  ..  ..   \n",
       "99789  #@이름#들 데이트폭력 근절해주세요 특히 애인 아내 죽이는 #@이름#에게 사형을 주...   0   0   0   \n",
       "99795  ''한남'''들은 페미, ''메갈''' 이런것도 안하는데 말하는거 들으면 ''토악질...   2   0   0   \n",
       "99839  ''헬조선''' ''새끼'''들은 인종차별이라고 왜 ''씨부리'''는지? 니네들도 ...   2   2   0   \n",
       "99905  시청자들이 ''갑'''인데 ''갑'''한테 ''그따구'''로 고지도 안하고 ''쳐맞...   0   3   0   \n",
       "99928                  놀고 ''자빠졌'''네ㅎㅎ#@이름# 콱! ''때려뿔'''라~   0   0   0   \n",
       "\n",
       "       폭력위협/범죄조장  성혐오  연령  인종/지역  장애  종교  정치성향  직업  정상  \n",
       "38             2    0   0      0   0   0     0   0   0  \n",
       "44             2    0   0      0   0   0     1   0   0  \n",
       "53             2    0   0      0   0   0     0   0   0  \n",
       "74             1    0   0      0   0   0     0   0   0  \n",
       "79             1    0   0      0   0   0     0   0   0  \n",
       "...          ...  ...  ..    ...  ..  ..   ...  ..  ..  \n",
       "99789          1    0   0      0   0   0     0   0   0  \n",
       "99795          2    3   0      0   0   0     0   0   0  \n",
       "99839          2    0   0      2   0   0     0   0   0  \n",
       "99905          2    0   0      0   0   0     0   0   0  \n",
       "99928          2    0   0      0   0   0     0   0   0  \n",
       "\n",
       "[6559 rows x 13 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stage2_data의 칼럼명이 '폭력위협/범죄조장'인 데이터\n",
    "\n",
    "#정상이 아닌것을 추출\n",
    "abnormal_threats = stage2_data[stage2_data['정상'] == 0]\n",
    "abnormal_threats\n",
    "\n",
    "# 폭력위협/범죄조장 열의 값이 0이 아닌 행만 추출\n",
    "violence_threats = stage2_data[stage2_data['폭력위협/범죄조장'] > 0]\n",
    "violence_threats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858bcb2-8730-425a-a453-7fac85c9d174",
   "metadata": {},
   "source": [
    "### 특수 따옴표 패턴 내 단어 추출\n",
    "\n",
    "- 특수 따옴표 패턴 내 단어는 특정 욕설을 의미함. 추출해서 Amazon Bedrock Guardrails의 사용자 지정단어에 사용\n",
    "- 1글자의 경우 너무 많은 필터링이 되기 때문에 삭제\n",
    "- Guardrails의 경우 현재 10,000개의 필터를 제공, 50kb이하, 한개당 최대 3 phrase\n",
    "- Guardrails의 욕설필터를 최대한 막을 수 있는 사용자지정단어를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c24287a-23f8-4360-9b90-d4cc160a8374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제거된 1글자 단어 (380개):\n",
      "\n",
      "필터링 전: 7252개 단어\n",
      "필터링 후: 6872개 단어\n",
      "제거된 단어 수: 380개\n",
      "\n",
      "최종 차단 단어/구문 목록:\n",
      "     blocked_words\n",
      "1          #@이름#정권\n",
      "2               ??\n",
      "3              강아지\n",
      "4             깽깽이 \n",
      "5               나불\n",
      "...            ...\n",
      "7247         흙수저노예\n",
      "7248           흡혈기\n",
      "7249            희생\n",
      "7250          히로시마\n",
      "7251            힘든\n",
      "\n",
      "[6872 rows x 1 columns]\n",
      "\n",
      "필터링된 차단 목록이 'guardrail_filtered_block_list.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 특수 따옴표 패턴 내 단어 추출\n",
    "block_list = create_block_list_from_special_quotes(violence_threats, '문장')\n",
    "\n",
    "#1글자 단어 제거 함수\n",
    "filtered_block_list = remove_single_char_words(block_list)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\n최종 차단 단어/구문 목록:\")\n",
    "print(filtered_block_list)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "filtered_block_list.to_csv('guardrail_filtered_block_list.csv', index=False)\n",
    "print(\"\\n필터링된 차단 목록이 'guardrail_filtered_block_list.csv'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129599be-d5e6-436b-9489-48186d3d8125",
   "metadata": {},
   "source": [
    "### 특수 따옴표 패턴의 한국어 욕설중 구를 제외한 명사만을 추출\n",
    "- 다양하게 변경하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dad0b705-dab3-4dbd-b36d-79df2544f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Mecab\n",
    "import pandas as pd\n",
    "\n",
    "def extract_korean_nouns(block_list_df, min_length=2):\n",
    "    \"\"\"\n",
    "    차단 목록에서 한국어 명사만 추출하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    block_list_df (DataFrame): 차단 단어 목록이 있는 DataFrame\n",
    "    min_length (int): 추출할 명사의 최소 길이 (기본값: 2)\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: 한국어 명사만 포함된 차단 단어 목록 DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Okt 형태소 분석기 초기화 (또는 try-except로 Mecab 시도)\n",
    "        okt = Okt()\n",
    "    except:\n",
    "        print(\"Okt 초기화 실패, Mecab을 시도합니다...\")\n",
    "        try:\n",
    "            mecab = Mecab()\n",
    "        except:\n",
    "            print(\"주의: KoNLPy 형태소 분석기 초기화 실패. 명사 추출이 제한적일 수 있습니다.\")\n",
    "            # 형태소 분석기 없이 기본적인 필터링만 수행\n",
    "            return block_list_df\n",
    "\n",
    "    # 결과를 저장할 리스트\n",
    "    extracted_nouns = []\n",
    "    skipped_words = []\n",
    "    \n",
    "    # 각 단어에서 명사 추출\n",
    "    for word in block_list_df['blocked_words']:\n",
    "        if not isinstance(word, str):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Okt로 명사 추출 시도\n",
    "            nouns = okt.nouns(word)\n",
    "            \n",
    "            # 명사가 없거나 모두 짧은 경우, 원래 단어 유지 (욕설/비속어는 명사로 인식되지 않을 수 있음)\n",
    "            if not nouns or all(len(noun) < min_length for noun in nouns):\n",
    "                if len(word.replace(\" \", \"\")) >= min_length:\n",
    "                    extracted_nouns.append(word)\n",
    "                else:\n",
    "                    skipped_words.append(word)\n",
    "            else:\n",
    "                # min_length 이상 길이의 명사만 추가\n",
    "                valid_nouns = [noun for noun in nouns if len(noun) >= min_length]\n",
    "                extracted_nouns.extend(valid_nouns)\n",
    "                \n",
    "                # 추출된 명사가 없으면 원래 단어 추가 고려\n",
    "                if not valid_nouns and len(word.replace(\" \", \"\")) >= min_length:\n",
    "                    extracted_nouns.append(word)\n",
    "        except Exception as e:\n",
    "            print(f\"'{word}' 처리 중 오류 발생: {e}\")\n",
    "            # 오류 발생 시 원래 단어 유지\n",
    "            if len(word.replace(\" \", \"\")) >= min_length:\n",
    "                extracted_nouns.append(word)\n",
    "    \n",
    "    # 중복 제거 및 정렬\n",
    "    extracted_nouns = sorted(list(set(extracted_nouns)))\n",
    "    \n",
    "    # 결과 DataFrame 생성\n",
    "    result_df = pd.DataFrame(extracted_nouns, columns=['blocked_words'])\n",
    "    \n",
    "    # 통계 출력\n",
    "    print(f\"원본 단어 수: {len(block_list_df)}\")\n",
    "    print(f\"추출된 명사 수: {len(result_df)}\")\n",
    "    print(f\"건너뛴 단어 수: {len(skipped_words)}\")\n",
    "    \n",
    "    if skipped_words:\n",
    "        print(\"\\n건너뛴 단어 목록:\")\n",
    "        for word in skipped_words:\n",
    "            print(f\"- '{word}'\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b69316f-7502-42f6-99ee-6b406f9a3296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 단어 수: 6872\n",
      "추출된 명사 수: 5541\n",
      "건너뛴 단어 수: 0\n",
      "\n",
      "최종 명사 기반 차단 단어/구문 목록:\n",
      "     blocked_words\n",
      "0               ??\n",
      "1             깽깽이 \n",
      "2            덮어씌우기\n",
      "3               문가\n",
      "4               미쳐\n",
      "...            ...\n",
      "5536            흡혈\n",
      "5537            희롱\n",
      "5538            희생\n",
      "5539          히로시마\n",
      "5540            힘든\n",
      "\n",
      "[5541 rows x 1 columns]\n",
      "\n",
      "명사 기반 차단 목록이 'guardrail_noun_block_list.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 3. 한국어 명사 추출\n",
    "noun_block_list = extract_korean_nouns(filtered_block_list, min_length=2)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\n최종 명사 기반 차단 단어/구문 목록:\")\n",
    "print(noun_block_list)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "noun_block_list.to_csv('guardrail_noun_block_list.csv', index=False)\n",
    "print(\"\\n명사 기반 차단 목록이 'guardrail_noun_block_list.csv'로 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
